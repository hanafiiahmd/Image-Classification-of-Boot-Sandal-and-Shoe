# -*- coding: utf-8 -*-
"""Template_Submission_Akhir (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K5-MoIuXeQ9B1y7M9RkMPGunMBth1O_A

# Proyek Klasifikasi Gambar: Shoe, Sandal & Boot Image Classification
- **Nama:** Ahmad Hanafi
- **Email:** hanapiahmad07@gmail.com
- **ID Dicoding:** hanafiiahmd

## Import Semua Packages/Library yang Digunakan
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os, shutil
import zipfile
import random
from random import sample
import shutil
from shutil import copyfile
import pathlib
from pathlib import Path
import numpy as np
import pandas as pd
from tqdm.notebook import tqdm as tq
from PIL import Image
import skimage
from skimage import io
from skimage.transform import resize
from skimage.transform import rotate, AffineTransform, warp
from skimage import img_as_ubyte
from skimage.exposure import adjust_gamma
import cv2
from skimage.util import random_noise
import keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import tensorflow as tf
from tensorflow.keras import Model, layers
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.keras.optimizers import Adam, RMSprop, SGD
from tensorflow.keras.layers import InputLayer, Conv2D, SeparableConv2D, MaxPooling2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.applications import MobileNet
from tensorflow.keras.applications.densenet import DenseNet121
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

!pip install tensorflowjs

"""## Data Preparation

### Data Loading
"""

from google.colab import files
files.upload()

!mv kaggle\ \(3\).json kaggle.json

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!ls ~/.kaggle

!kaggle datasets download -d hasibalmuzdadid/shoe-vs-sandal-vs-boot-dataset-15k-images
!unzip shoe-vs-sandal-vs-boot-dataset-15k-images.zip -d dataset

import os
print(os.listdir('.'))

!ls dataset

!ls

import os

dataset_path = "dataset/Shoe vs Sandal vs Boot Dataset"

classes = os.listdir(dataset_path)
print("Kelas yang ditemukan:", classes)

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import random

fig, axs = plt.subplots(len(classes), 5, figsize=(15, 10))

for row, class_name in enumerate(classes):
    class_path = os.path.join(dataset_path, class_name)
    images = os.listdir(class_path)
    selected_images = random.sample(images, 5)

    for col, img_name in enumerate(selected_images):
        img_path = os.path.join(class_path, img_name)
        img = mpimg.imread(img_path)

        axs[row, col].imshow(img)
        axs[row, col].axis('off')
        axs[row, col].set_title(class_name)

plt.tight_layout()
plt.show()

import os
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

dataset_path = "dataset/Shoe vs Sandal vs Boot Dataset"


file_name = []
labels = []
full_path = []

for path, subdirs, files in os.walk(dataset_path):
    for name in files:
        full_path.append(os.path.join(path, name))
        labels.append(path.split('/')[-1])
        file_name.append(name)

df = pd.DataFrame({"path": full_path, "file_name": file_name, "labels": labels})

print(df.head())

plt.figure(figsize=(6,6))
sns.set_style("darkgrid")
sns.countplot(x=df["labels"])
plt.title("Distribusi Gambar di Setiap Kelas")
plt.xlabel("Kelas")
plt.ylabel("Jumlah Gambar")
plt.show()

from PIL import Image
import os

dataset_path = "dataset/Shoe vs Sandal vs Boot Dataset"

image_sizes = {}

for class_name in os.listdir(dataset_path):
    class_path = os.path.join(dataset_path, class_name)

    if os.path.isdir(class_path):
        for img_name in os.listdir(class_path):
            img_path = os.path.join(class_path, img_name)

            try:
                with Image.open(img_path) as img:
                    size = img.size

                    if size in image_sizes:
                        image_sizes[size] += 1
                    else:
                        image_sizes[size] = 1
            except:
                print(f"Error membaca {img_path}")

print("Resolusi gambar yang ditemukan dalam dataset:")
for size, count in image_sizes.items():
    print(f"Resolusi {size}: {count} gambar")

"""### Data Preprocessing"""

import os
import cv2
import numpy as np
from tqdm.notebook import tqdm as tq
from skimage.transform import resize

# Path dataset asli
dataset_path = "dataset/Shoe vs Sandal vs Boot Dataset"

# Path untuk menyimpan hasil preprocessing
preprocessed_path = "Preprocessed_Dataset"
os.makedirs(preprocessed_path, exist_ok=True)

# Ukuran gambar yang diinginkan
IMG_SIZE = (128, 128)

# Looping setiap kelas (kategori)
for class_name in os.listdir(dataset_path):
    class_path = os.path.join(dataset_path, class_name)
    save_path = os.path.join(preprocessed_path, class_name)
    os.makedirs(save_path, exist_ok=True)

    # Looping setiap gambar dalam kelas
    for img_name in tq(os.listdir(class_path), desc=f"Processing {class_name}"):
        img_path = os.path.join(class_path, img_name)
        img = cv2.imread(img_path)
        if img is None:
            continue

        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Konversi ke RGB
        img = resize(img, IMG_SIZE)  # Resize gambar
        img = img.astype(np.float32) / 255.0  # Normalisasi ke [0,1]

        # Simpan gambar yang telah diproses
        save_img_path = os.path.join(save_path, img_name)
        cv2.imwrite(save_img_path, (img * 255).astype(np.uint8))

import numpy as np

img = np.clip(img, 0, 1)

print(f"Tipe Data: {img.dtype}, Shape: {img.shape}")

import os
import cv2
import numpy as np
import random
from tqdm.notebook import tqdm as tq
from tensorflow.keras.preprocessing.image import ImageDataGenerator

preprocessed_path = "Preprocessed_Dataset"

augmented_path = "Augmented_Dataset"
os.makedirs(augmented_path, exist_ok=True)

augmentasi_persen = 0.2  # Augmentasi 20% dari gambar per kelas

augmentasi = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True
)

# Loop setiap kelas dalam dataset
for class_name in os.listdir(preprocessed_path):
    class_path = os.path.join(preprocessed_path, class_name)
    save_path = os.path.join(augmented_path, class_name)
    os.makedirs(save_path, exist_ok=True)

    img_list = os.listdir(class_path)
    total_img = len(img_list)
    num_augmented = max(10, int(total_img * augmentasi_persen))

    # Pilih gambar secara acak untuk diaugmentasi
    selected_imgs = random.sample(img_list, num_augmented)

    for img_name in tq(selected_imgs, desc=f"Augmenting {class_name}"):
        img_path = os.path.join(class_path, img_name)
        img = cv2.imread(img_path)
        if img is None:
            continue

        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Konversi ke RGB
        img = np.expand_dims(img, axis=0)  # Tambahkan dimensi batch
        aug_iter = augmentasi.flow(img, batch_size=1)  # Generate gambar baru

        # Simpan 1 versi augmentasi dari gambar ini
        aug_img = next(aug_iter)[0] * 255
        aug_img = aug_img.astype(np.uint8)
        save_img_path = os.path.join(save_path, f"aug_{img_name}")
        cv2.imwrite(save_img_path, aug_img)

import matplotlib.pyplot as plt
import random

# Pilih salah satu kelas secara acak
augmented_classes = os.listdir(augmented_path)
chosen_class = random.choice(augmented_classes)
class_path = os.path.join(augmented_path, chosen_class)

# Pilih beberapa gambar dari kelas tersebut
augmented_images = random.sample(os.listdir(class_path), min(5, len(os.listdir(class_path))))

# Tampilkan gambar hasil augmentasi
fig, axes = plt.subplots(1, len(augmented_images), figsize=(15, 5))
fig.suptitle(f"Hasil Augmentasi - Kelas: {chosen_class}", fontsize=16)

for i, img_name in enumerate(augmented_images):
    img = cv2.imread(os.path.join(class_path, img_name))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    axes[i].imshow(img)
    axes[i].axis("off")
    axes[i].set_title(img_name)

plt.show()

"""#### Split Dataset"""

import os
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing import image

# Path dataset hasil preprocessing
preprocessed_path = "Preprocessed_Dataset"

# Buat list untuk menyimpan path gambar dan label
image_paths = []
labels = []
classes = os.listdir(preprocessed_path)

for class_name in classes:
    class_path = os.path.join(preprocessed_path, class_name)
    for img_name in os.listdir(class_path):
        image_paths.append(os.path.join(class_path, img_name))
        labels.append(class_name)

# Konversi label ke format numerik
label_to_index = {class_name: idx for idx, class_name in enumerate(classes)}
numeric_labels = [label_to_index[label] for label in labels]

# Split dataset (80% train, 10% validation, 10% test)
X_train, X_temp, y_train, y_temp = train_test_split(image_paths, numeric_labels, test_size=0.2, stratify=numeric_labels, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

from tensorflow.keras.preprocessing.image import load_img, img_to_array
import numpy as np

def load_images(image_paths):
    images = [img_to_array(load_img(img_path, target_size=(128, 128))) / 255.0 for img_path in image_paths]
    return np.array(images)

# Konversi X_train, X_val, X_test dari path menjadi array gambar
X_train = load_images(X_train)
X_val = load_images(X_val)
X_test = load_images(X_test)

# Konversi label menjadi array numpy
y_train = np.array(y_train)
y_val = np.array(y_val)
y_test = np.array(y_test)

"""## Modelling"""

from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Definisikan callback
callbacks = [
    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-6),
    EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)
]

# Bangun model CNN
model = Sequential([
    Conv2D(32, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(len(classes), activation='softmax')
])

# Kompilasi model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Latih model dengan callback
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,
    batch_size=32,
    callbacks=callbacks
)

# Evaluasi model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {accuracy * 100:.2f}%')

"""## Evaluasi dan Visualisasi"""

# Prediksi kelas untuk confusion matrix
y_pred = np.argmax(model.predict(X_test), axis=1)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
class_report = classification_report(y_test, y_pred, target_names=classes)
print("Classification Report:\n", class_report)

# Plot akurasi dan loss
plt.figure(figsize=(12, 5))

# Plot Akurasi
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')
plt.legend()

# Plot Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Model Loss')
plt.legend()

plt.show()

"""## Konversi Model"""

# Simpan model dalam berbagai format

#Saved Model
save_path = '/content/models/'
saved_model_path = os.path.join(save_path, 'saved_model')
os.makedirs(saved_model_path, exist_ok=True)
tf.saved_model.save(model, saved_model_path)

model.save("model.h5")  # Format h5

# Konversi ke TensorFlow Lite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
with open("model.tflite", "wb") as f:
    f.write(tflite_model)

# Konversi ke TensorFlow.js
!tensorflowjs_converter --input_format=keras model.h5 tfjs_model

"""## Inference (Optional)"""

import tensorflow as tf
import numpy as np
from PIL import Image

# Load model dari SavedModel
saved_model_path = "/content/models/saved_model"
model = tf.saved_model.load(saved_model_path)
infer = model.signatures["serving_default"]

# Load dan preprocess gambar
image_path = "g8041_EXTRALARGE.jpg"
img = Image.open(image_path).convert("RGB")
img = img.resize((128, 128))
img_array = np.array(img, dtype=np.float32) / 255.0

batch_size = 32
dummy_batch = np.zeros((batch_size, 128, 128, 3), dtype=np.float32)
dummy_batch[0] = img_array

# Lakukan prediksi
output_data = infer(inputs=tf.constant(dummy_batch))

output_data = output_data['output_0'].numpy()[0]

class_labels = ["Shoe", "Sandal", "Boot"]
predicted_class = class_labels[np.argmax(output_data)]

print(f"Output prediksi: {predicted_class}")

import tensorflow as tf
import numpy as np
from PIL import Image

# Load model dari SavedModel
saved_model_path = "/content/models/saved_model"
model = tf.saved_model.load(saved_model_path)
infer = model.signatures["serving_default"]

# Load dan preprocess gambar
image_path = "BK_05170_BRN2.jpg"
img = Image.open(image_path).convert("RGB")
img = img.resize((128, 128))
img_array = np.array(img, dtype=np.float32) / 255.0

batch_size = 32
dummy_batch = np.zeros((batch_size, 128, 128, 3), dtype=np.float32)
dummy_batch[0] = img_array

# Lakukan prediksi
output_data = infer(inputs=tf.constant(dummy_batch))

output_data = output_data['output_0'].numpy()[0]

class_labels = ["Shoe", "Sandal", "Boot"]
predicted_class = class_labels[np.argmax(output_data)]

print(f"Output prediksi: {predicted_class}")